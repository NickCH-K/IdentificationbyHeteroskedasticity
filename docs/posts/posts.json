[
  {
    "path": "posts/2022-05-17-fun-with-parallel-trends/",
    "title": "Fun With Parallel Trends",
    "description": "What havoc can we wreak when we realize that parallel trends is but a functional form assumption?",
    "author": [
      {
        "name": "Nick Huntington-Klein",
        "url": {
          "https://www.nickchk.com": {}
        }
      }
    ],
    "date": "2022-05-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nParallel Trends\r\nWe Do What We Like\r\nI Will Punch It Until It\r\nIs True\r\nA Free Lunch?\r\n\r\nDifference-in-differences\r\n(DID) is a fantastic and widely-applied tool in the causal inference\r\ntoolbelt. The idea is simple: you want to know if some sort of\r\ntreatment, which was applied at time \\(T\\) to a “treatment group”, had an effect.\r\nYou start by looking at whoever was treated and compare them after-\\(T\\) to before-\\(T\\), calculating the before-after\r\n“difference.”\r\nHowever, we can’t just assume that the before-after difference is the\r\neffect of the treatment, since maybe things would have just changed over\r\ntime anyway. We have a time effect clogging things up, and the\r\nbefore-after difference is a combination of the treatment effect and the\r\ntime effect.\r\nSo we take a control group that didn’t get the policy and\r\nlook at their before-after difference. We assume that\r\ntheir before-after difference is the before-after difference\r\nthe treatment group would have gotten if the treatment had\r\nnever occurred, i.e. this is our estimate of the time effect alone. We\r\nsubtract out this control group before-after difference (the time\r\neffect) from the treatment group before-after difference (time effect +\r\ntreatment effect), leaving us just with the treatment effect. And now we\r\nknow the effect of treatment.\r\nParallel Trends\r\nThe key assumption holding this whole method together is parallel\r\ntrends. We can describe parallel trends in all sorts of fancy ways,\r\nlike “we assume that, if treatment had never occurred, the gap between\r\ntreated and control groups would have remained constant from the\r\nbefore-treatment period to the after-treatment period.”\r\nBut really what parallel trends means is this: remember that “time\r\neffect” we talked about? Parallel trends says that the “time\r\neffect” for the treatment group is the same as the “time effect” for the\r\ncontrol group. So when we subtract out the control group’s\r\nbefore-after difference, we’re subtracting the “time effect”, how much\r\nthe treatment group would have changed over time anyway.\r\nNow here’s the wild thing about parallel trends: it feels\r\nlike you’re making some deep assumption about something structural and\r\nmeaningful, something about the comparability of your treatment and\r\ncontrol groups. But nope! It’s actually just a functional form\r\nassumption.\r\nIn fact, whether parallel trends is true or not is entirely\r\nbased on which functional form you choose. This pops up most\r\ncommonly in the decision of whether or not to take the log of your\r\ndependent variable in DID, since doing so could either fix or break your\r\nparallel trends assumption (or it could be broken either way).\r\nFor example, imagine that in the pre-treatment period, the treatment\r\ngroup has \\(Y = 2\\) and the control\r\ngroup has \\(Y = 1\\). In the\r\npost-treatment period, in the unobserved counterfactual where\r\ntreatment didn’t occur, the treatment group has \\(Y = 3\\) and the control group has \\(Y = 2\\).\r\nIf the treatment hadn’t occurred, both groups would have increased by\r\nexactly 1 unit from before to after. Parallel trends holds! But what if\r\nwe take the natural log of \\(Y\\)? Well,\r\nnow, the treatment group increases by \\(\\ln(3)-\\ln(2)\\approx .405\\) and the control\r\ngroup increases by \\(\\ln(2)-\\ln(1)\\approx\r\n.693\\). Oops! Parallel trends is blown.\r\nWe Do What We Like\r\nSo parallel trends is a functional form assumption. So what?\r\nSo, that means it always holds. For the right functional\r\nform.\r\nLike I said, it’s just a functional form assumption, not anything\r\ndeep and structural about the data generating process. If we pick the\r\nright transformation, we can make parallel trends hold.\r\nThanking Ben\r\nJackson for this particular specification, let’s pick the following\r\ntransformation:\r\n\\[ F(Y, a) = (Y+a)^2 \\]\r\nNow, let’s specify our parallel trends assumption for a two-period\r\nDID. Let’s call \\(Y_{gt}\\) the outcome\r\nfor group \\(g\\) ((T)reated or\r\n(C)ontrol) in time period \\(t\\)\r\n((B)efore or (A)fter treatment).\r\n\\[ (F(Y_{TA},a) - F(Y_{TB},a) =\r\nF(Y_{CA},a) - F(Y_{CB},a)) \\] which we can solve for \\(a\\)! We get:\r\n\\[ a = \\frac{Y_{TB}^2+Y_{CA}^2 - Y_{TA}^2\r\n- Y_{CB}^2}{2(Y_{TA}+Y_{CB}-Y_{TB}-Y_{CA})} \\] So as long as\r\n\\(Y_{TA}+Y_{CB}-Y_{TB}-Y_{CA} \\neq 0\\)\r\n(in other words, as long as the treated group didn’t rise from before to\r\nafter by the exact same amount that the control group fell), we can\r\nalways pick an \\(a\\) that gives us a\r\ntransformation to make parallel trends true. Of course, actually picking\r\nthe right \\(a\\) requires that we know\r\nthe counterfactual \\(Y_{TA}\\), but\r\nwe’ll get there later.\r\nLet’s work a quick example. Let’s say that, in the before-treatment\r\nperiod, \\(Y_{CB} = 1\\) and \\(Y_{TB} = 2\\). Then, in the after-treatment\r\nperiod, \\(Y_{CA} = 10\\) and \\(Y_{TA} = 8\\). The control group rose by 9,\r\nand if treatment hadn’t occurred, the treated group would have risen by\r\nonly 6. Parallel trends broken!\r\nBut using the formula above we can calculate \\(a = -13/2\\) and get our transformed values\r\n\\(F(Y_{CB},a) = 121/4\\), \\(F(Y_{CA},a) = 49/4\\), \\(F(Y_{TB},a) = 81/4\\), and \\(F(Y_{TA},a) = 9/4\\). Does parallel trends\r\nhold? We need\r\n\\[ \\frac{9}{4} - \\frac{81}{4} =\r\n\\frac{49}{4} - \\frac{121}{4} \\]\r\nfor which you’ll find both sides are exactly \\(-18\\). Parallel trends holds.\r\nI Will Punch It Until It Is\r\nTrue\r\nSo if we did know what the counterfactuals were, we could\r\nalways pick an \\(a\\) that would make\r\nparallel trends true, if we used transformation \\(F(x,a) = (x+a)^2\\).\r\nWhat if we don’t?\r\nWell, dang, let’s just try a bunch of \\(a\\) values and see what we get! It sure\r\nwould be interesting if we could just guess some \\(a\\) values from \\(-5\\) to \\(5\\) and get pretty close.\r\nLet’s start by creating four random counterfactuals. For simplicity\r\nlet’s keep them uniformly distributed in the range of 0 to 1. Then,\r\nlet’s add a true treatment effect of .2. We’ll run things a bunch of\r\ntimes, picking a range of \\(a\\) values\r\nfrom -5 to 5 and see what we get. Wouldn’t it be wild if we tended to\r\nget accurate answers of .2 anyway?\r\n\r\n\r\n\r\nUm… what? To be entirely honest, I didn’t expect it to actually work.\r\nBut look at that. I just picked a range of random \\(a\\) values to do the transformation on. And\r\nrepeating the process I… I get a normal-looking distribution centered\r\naround the truth of \\(.2\\). Huh?\r\nOkay, I’m cheating a little bit. The DID estimated using our\r\ntransformation gives us an estimate on \\(F(Y,a)\\) scale, not on the \\(Y\\) scale where the truth is \\(.2\\). To get back to that scale I had to do\r\na re-conversation, and it turns out that converting the DID estimate in\r\n\\(F(Y,a)\\) scale back to an estimate in\r\n\\(Y\\) scale leads you to a quadratic\r\nformula, so there are actually two solutions to what the DID estimate is\r\nin terms of \\(Y\\), and I picked the\r\nestimate closer to \\(.2\\). Cheating!\r\nSorry. Let’s go ahead and undo that lil problem and look at both\r\nquadratic solutions:\r\n\r\n\r\n\r\nThat still looks pretty good. Each of the solutions has one fat tail,\r\nbut it’s still not bad. Um, this blog post wasn’t supposed to go this\r\nwell. What happens if we average the two solutions for each run?\r\n\r\n\r\n\r\nNow we get something that looks ugly, as it should be. Sure, the\r\ndistribution is centered around the truth, but there’s no peak at the\r\ntruth, in fact there’s a weird divot.\r\nA Free Lunch?\r\nBut still, it sort of looks like if we knew which of the two\r\nquadratic solutions to pick, we’d have the ability to pick a functional\r\nform that made parallel trends true, and the ability to recover the\r\noriginal-value-scaled estimate, with a normal-looking sampling\r\ndistribution centered around the true value.\r\nSeems like in a lot of cases we could probably eliminate one of the\r\ntwo quadratic solutions by theory, say if one was positive and one\r\nnegative (not guaranteed to happen). Hmm.\r\nI’ll be honest, I’ve had a lot of ideas for\r\ncausal-identification-by-magic in the past, and none of them have borne\r\nout. I expected this blog post to be the same. And yet! This is still\r\nlooking pretty good so far. I’m sure there’s a reason bubbling just\r\nunder the surface as to why this wouldn’t actually work (perhaps I’ve\r\ngotten lucky with the range of \\(a\\)\r\nvalues I picked) but for the moment I am keeping the magic alive and\r\nwondering what exactly about this must not work, so that I’m\r\njust fooling myself.\r\nOr maybe not?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-17-fun-with-parallel-trends/fun-with-parallel-trends_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-17T04:15:28-07:00",
    "input_file": "fun-with-parallel-trends.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-13-ols-translated-into-english/",
    "title": "Ordinary Least Squares, Translated into English",
    "description": "Going through the logic of what OLS actually is and does, in plain-language terms.",
    "author": [
      {
        "name": "Nick Huntington-Klein",
        "url": {
          "https://www.nickchk.com": {}
        }
      }
    ],
    "date": "2022-02-16",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOLS Picks a Shape\r\nHow OLS Picks a Shape\r\nThe Shape OLS Picks\r\nOLS has an Error\r\nOLS Can be Biased\r\nThe Precision of OLS\r\n\r\nOrdinary least squares (OLS) is a statistical algorithm that uses a set of explanatory variables to predict a dependent variable. I dunno, maybe you’ve heard of it.\r\nDescribing what OLS does and what properties it has is often done in the form of mathematical statements. These statements are precise, and correct, and often confusing.\r\nIn this post I will translate several of these mathematical statements into the English language, giving a sense of what they actually mean in a non-technical sense. Sometimes I’ll add a graph if it helps.\r\nMany of these translations can also be found on the more extended discussions of OLS in my book The Effect. So check that out. But I’ve decided to collect, or perhaps reiterate, some of them here. I’m not referring back to the book as I do this. If I have any here that I didn’t include in the book, oops.\r\nAs a note, this is all about translating precise mathematical statements into good ol’ imprecise English. Yes, these translations will miss some of the details. That’s in their nature. And if I included every caveat they would only become confusing. So no need to DM me about some caveat that the English translation leaves out. If you want full precision, just revert to the mathematical statements. If you just want a head start on understanding what OLS, uh, is, with the knowledge that some finger details may emerge later, forge on.\r\nOLS Picks a Shape\r\nOLS is a tool that picks the best version of a shape. That’s all it does. That is, we tell it the kind of shape we want it to use, and we want it to tell us how to angle and move that shape to best fit the data.\r\n\r\n\r\n\r\nWhat does that mean mathematically and in English? Mathematically, we might write down a regression equation that looks like this:\r\n\\[ Y = \\beta_0 + \\beta_1X+\\varepsilon \\]\r\nIn doing so, we are saying “OLS, give me the best straight line that describes my data,” since this equation describes a straight line with an intercept (\\(\\beta_0\\)) and a slope (\\(\\beta_1\\)). If we didn’t want a straight line - if we wanted a curvy line or something else, we’d need to rewrite this equation.\r\nThis also tells us how we can interpret our regression coefficients when we get them. They’re just describing a line, so we can interpret the coefficients as telling us how to move along that line.\r\nSo the slope coefficient, \\(\\beta_1\\), is just how steep the line is. Big positive numbers indicate a steep slope upwards, big negative numbers indicate a steep slope downwards, and numbers near zero indicate a flat line. More precisely, “a one-unit increase in \\(X\\) is associated with a \\(\\beta_1\\)-unit increase in \\(Y\\).”\r\n\r\n\r\n\r\nHow OLS Picks a Shape\r\nSo we give OLS a shape and it picks the best version of that shape. How does it pick it? OLS picks coefficents that minimize the sum (\\(\\sum\\)) of squared (\\(^2\\)) residuals (\\(Y-\\hat{Y}\\)). With a single predictor \\(X\\) this means:\r\n\\[ \\hat{\\beta} = \\text{argmin}_{\\hat{\\beta}}\\sum_i(Y_i-\\hat{Y}_i)^2 = \\text{argmin}_{\\hat{\\beta}}\\sum_i(Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1X_i) \\]\r\nwhere \\(i\\) corresponds to the different observations. \\(\\hat{Y}_i\\) is our prediction of the outcome variable \\(Y\\) for observation \\(i\\). If take \\(X_i\\) and look for where that \\(X\\) value falls on our OLS line (\\(\\hat{\\beta}_0+\\hat{\\beta}_1X\\)), that’s how we get our prediction \\(\\hat{Y}\\). That prediction will, naturally, not be perfect, and so we’ll have a residual, which is the difference between the actual outcome \\(Y\\) and the prediction \\(\\hat{Y}\\). We square those residuals (\\((Y_i-\\hat{Y}_i)^2\\)), sum them up across all the observations (\\(\\sum_i\\)), and then pick the line that makes that sum of squared residuals as small as possible (\\(\\text{argmin}_{\\hat{\\beta}}\\)).\r\nSo what does this mean in English? It means that OLS is trying to pick the best line for you. Its idea of “best” is a line that makes predictions as close to accurate as possible. Since it can’t hit all the points perfectly, it has to trade off all the different errors in prediction for all its different predictions. It squares the errors in prediction, which both (a) makes sure too-high and too-low predictions are both bad, and (b) makes it try extra-hard to avoid predictions that are way off.\r\nThe Shape OLS Picks\r\nIf you solve the minimization problem in the previous section, you get the following estimate for the slope and intercept coefficients:\r\n\\[ \\hat{\\beta}_1 = \\frac{Cov(X,Y)}{Var(X)} \\] \\[ \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} \\]\r\nwhere \\(Cov(X,Y)\\) is the covariance between \\(X\\) and \\(Y\\), \\(Var(X)\\) is the variance of \\(X\\), and \\(\\bar{Y}\\) and \\(\\bar{X}\\) are the mean of \\(Y\\) and \\(X\\), respectively.\r\nIf we have more than one predictor, then with a matrix of predictors \\(X\\) and the associated coefficient vector \\(\\hat{\\beta}\\) we have\r\n\\[ \\hat{\\beta} = (X'X)^{-1}X'Y \\]\r\nWhy does it make sense that these are the calculations we get? Let’s translate.\r\nFirst, the slope of \\(\\hat{\\beta}_1 = Cov(X,Y)/Var(X)\\). There are a few ways we can think of this. It helps to first translate what “divides by” means. In statistical applications, \\(A/B\\) can usually be thought of as “how much \\(A\\) is there for each \\(B\\)?” or “out of all the \\(B\\)s, how many are \\(A\\)s?”\r\nOne way is “out of all the variation in \\(X\\) (\\(/Var(X)\\)), how much of that goes along with \\(Y\\) (\\(Cov(X,Y)\\))?” I think this translation is highly intuitive as a first step, although it becomes less so when you realize that \\(\\hat{\\beta}_1\\) can be outside of the range of 0 to 1 and so the implied-but-not-stated idea that this is a proportion isn’t accurate.1\r\nAnother way is “how much change in \\(Y\\) (\\(Cov(X,Y)\\)) can we expect relative to (\\(/\\)) how much \\(X\\) is changing (\\(Var(X)\\))? How much rise up the y-axis do we get as we run along the x-axis?\r\nWhichever of these two translations you favor, their logic works just as well for the multivariable version \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\). The inverse (\\(^{-1}\\)) just means “divide by”.\r\nOh, and don’t forget the intercept \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\). This one’s easy. It forces the average of \\(Y\\), \\(\\bar{Y}\\), to be the same as the average prediction of the model. In other words, it says “hey OLS, whatever slope you think is best, I trust you. I’m just gonna go ahead and make sure that the predictions are right on average.”\r\nOLS has an Error\r\nA fully specified equation that you might use to set up a regression should always have an error term, which here is \\(\\varepsilon\\):\r\n\\[ Y = \\beta_0 + \\beta_1X+\\varepsilon \\]\r\nThis error term is not the same as the residual. The residual is what we get if we actually get a sample of data, estimate our OLS coefficients, and compare the OLS predictions against the actual \\(Y\\) values. The error, on the other hand, is the difference between the actual \\(Y\\) value and what the OLS would predict if we had full population data.\r\nAn effective, if wordy, translation is this: “In truth, there’s a whole bunch of stuff that determines the value of \\(Y\\). The model we write out is telling us literally everything about what determines \\(Y\\) (that’s why it’s \\(Y=\\), not”\\(Y\\) is related to…“). Our model contains some stuff (\\(X\\)), but leaves out other stuff. That other stuff has to be somewhere, so it must be in the error (\\(\\varepsilon\\)).”\r\nOLS Can be Biased\r\nIf you want the estimated coefficient \\(\\hat{\\beta}_1\\) from \\(Y = \\beta_0 + \\beta_1X+\\varepsilon\\) to reflect “the effect of \\(X\\) on \\(Y\\),” then we must assume that \\(E(X\\varepsilon) = 0\\), where \\(E()\\) is the expectations function. \\(E(X\\varepsilon) = 0\\) means that there’s no linear relationship between \\(X\\) and \\(\\varepsilon\\).\r\nIf that’s not true, and \\(E(X\\varepsilon) \\neq 0\\), then we have omitted variable bias. Some variable that’s related to \\(X\\) has been omitted from the model and thus is in \\(\\varepsilon\\) (remember last section? Every determinant of \\(Y\\) that’s not in the model is in \\(\\varepsilon\\)).\r\nIf we estimate a regression based on the model \\(Y = \\beta_0 + \\beta_1X+\\varepsilon\\), but there’s a variable \\(Z\\) in the error term \\(\\varepsilon\\), the estimate we get will be\r\n\\[ \\hat{\\beta}_1 = \\beta_1 + \\beta_Z\\beta_{XZ} \\]\r\nwhere \\(\\beta_Z\\) is the coefficient we’d get on \\(Z\\) if we instead estimated a regression of \\(Y\\) on both \\(X\\) and \\(Z\\), and \\(\\beta_{XZ}\\) is the coefficient we’d get on \\(Z\\) if we regressed \\(X\\) on \\(Z\\).\r\nHow can we translate this?\r\n“If \\(Z\\) tends to hang around \\(X\\) (\\(\\beta_{XZ} > 0\\)), but OLS doesn’t know about it (\\(Z\\) isn’t in the model), then \\(X\\) will get the credit for everything \\(Z\\) actually did (\\(\\beta_Z\\)).”\r\nThe same idea holds if \\(X\\) and \\(Z\\) are negatively related (\\(\\beta_{XZ} < 0\\)), although it’s not quite as intuitive:\r\n“If \\(Z\\) tends to disappear when \\(X\\) is around (\\(\\beta_{XZ} < 0\\)), but OLS doesn’t know about it (\\(Z\\) isn’t in the model), then \\(X\\) will get blamed for \\(Z\\) not doing what it normally does (\\(\\beta_Z\\)).”\r\nThe Precision of OLS\r\nSince we often want to use our OLS estimates to make inferences about the population, we want to have an idea of what the sampling distribution of OLS is.\r\nIn the regression following from the model \\(Y = \\beta_0+\\beta_1X+\\varepsilon\\), if we assume that \\(\\varepsilon\\) has a constant variance \\(\\sigma^2\\), then the sampling distribution of our estimate \\(\\hat{\\beta}_1\\) is centered around the population value of that coefficient \\(\\beta_1\\), and has a standard deviation of\r\n\\[ se(\\hat{\\beta}_1) = \\sqrt{\\frac{\\sigma^2}{Var(X)}} \\]\r\nor in the multivariable context, the very similar\r\n\\[ Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1} \\]\r\nwhere \\(se(\\hat{\\beta}_1)\\) is then the square root of the first element of the trace of \\(Var(\\hat{\\beta})\\).\r\nHow can we interpret this? Well, we have a standard error that grows the more the error term varies (\\(\\sigma^2\\)), and shrinks the more \\(X\\) varies (\\(/Var(X)\\)). The error term contains all the other stuff going on with \\(Y\\) other than what’s in the model. So we can say “The difficulty in figuring out exactly what our slope coefficient is is based on how much other stuff there is going on with \\(Y\\) (\\(\\sigma^2\\)) relative to the parts we’re actually interested in (\\(Var(X)\\)). The more noise there is relative to signal, the less we can see, and the less precise we’ll be.”\r\nThere’s another few steps (and another translation) to go here for when we want to get a little dirty. First, keep in mind that whatever the standard error is, we don’t actually know it, we have to estimate it just like we have to estimate the coefficient itself. We can make an estimate of \\(\\hat{\\sigma}^2\\) using the variance of the residuals. Second I gave the equations for standard errors if the error term has a constant variance. However, if we don’t make that assumption, then instead of the nice simple \\(\\sigma^2\\) we have \\(E(\\varepsilon\\varepsilon')\\), which is the variance-covariance matrix of the error terms.2 Estimating the standard error, then, means we have to try to estimate \\(E(\\varepsilon\\varepsilon')\\) in its entirety,3 Things like heteroskedasticity-robust or cluster-robust standard error estimations make different assumptions about \\(E(\\varepsilon\\varepsilon')\\) in order to more precisely estimate it.\r\nWhat does this mean?\r\n“How precise our estimate is depends on how much noise there is. If we’re wrong about how much noise there is, we’ll also be wrong about how precise our estimate is. If the noise isn’t just a single thing but instead changes a lot, we have to do something to figure out how it changes, or we’ll be wrong.”\r\n\r\nAlthough this same interpretation works much better for Pearson correlation, which is basically a univariate regression coefficient rescaled so it’s much more proportion-like.↩︎\r\nThe entire equation itself gets a bit gnarlier too, since \\(\\sigma^2\\) lets some stuff cancel out and we don’t get that any more. In the multivariable context it’s \\((X'X)^{-1}(X'E(\\varepsilon\\varepsilon')X)(X'X)^{-1}\\).↩︎\r\nOr at least that’s what sandwich-based estimators do.↩︎\r\n",
    "preview": "posts/2022-02-13-ols-translated-into-english/ols-translated-into-english_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-02-16T09:49:21-08:00",
    "input_file": "ols-translated-into-english.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-23-overdebunked/",
    "title": "Overdebunked! Six Statistical Critiques That Don't Quite Work",
    "description": "When healthy skepticism of statistics turns into worse statistics (and an excuse).",
    "author": [
      {
        "name": "Nick Huntington-Klein",
        "url": {
          "https://www.nickchk.com": {}
        }
      }
    ],
    "date": "2022-01-23",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Bias in Levels vs. Bias in Changes\r\n2. Demanding Infinite Detail\r\n3. Asking for Double-Counting\r\n4. Misunderstanding Control Variables\r\n5. Correlation Can’t be Causation\r\n6. Giving No Slack Whatsoever\r\n\r\nStatistical results and data analyses are quite often wrong. Sometimes they’re wrong because of carelessness, sometimes they’re wrong even though we cared a lot because it’s just really hard to get them right, and other times they’re wrong on purpose. It shouldn’t shock anyone to hear this. This is the view that a lot of people have about statistics. It’s not a surprise to me that the best-selling statistics book of all time is all about how statistics can be wrong, the classic How to Lie with Statistics by Darrell Huff. I’m not sure whether Huff inspired all this skepticism or simply reflected it back at us (I suspect the latter), but regardless we know to be on the lookout for bad statistics.\r\n\r\n\r\n\r\nFigure 1: How to Lie with Statistics\r\n\r\n\r\n\r\nSkepticism about statistics is good. However, just as blind belief can lead you to believe things that aren’t true, an overabundance of skepticism can lead you to disbelieve things that are true. As author, podcaster, and modern be-careful-with-data industry member Tim Harford recounts, Darrell Huff took the data skepticism he was known for a little too far. He ended up getting paid by the tobacco industry to testify before congress about just how darn safe cigarettes were. After all, Huff pointed out that statistics can be misrepresented. Surely it follows that they always are. All those statistics showing that cigarettes are harmful for you can’t be trusted either, right?\r\nSo if we’re going to be skeptical of statistical results (and we should be), we have to be careful that we’ve got actual criticisms that make sense.\r\nOne interesting feature of spending time on social media is that you get to see all sorts of people make criticisms of statistical results. A lot of these criticisms don’t really make much sense. Or sometimes they’re legit criticisms in general that don’t really apply to the thing being criticized. Or they’ve stumbled on an actual problem but then assign way too much importance to it.\r\nIt seems from my end of things that most of these critiques are driven by a desire for the result they’re criticizing to be wrong, rather than actual concern for statistical technique. So maybe talking about the statistical side of things is pointless. But also some people are genuinely interested in whether the stats are right are not, and in any case talking about some of these misconceptions may be handy.\r\nSo, below, I’ve listed six statistical critiques I commonly see on social media, and why they’re not great critiques. This isn’t a comprehensive compendium of statistical mistakes, but rather things I’ve seen over and over reading the comments on data-containing posts about climate change, political polling, COVID, violence, foreign relations, etc. etc. etc.. These aren’t technical errors - they’re not about misinterpreting a \\(p\\)-value or whatever, but more about common-sense critiques of published statistical results that anyone could make. I can guarantee you that these mistakes aren’t limited to any one side of any given issue (in fact, some of the examples below are of people who almost certainly agree with me on what the actual truth is, even though I think they’ve made a mistake getting there). You, yes you, may even remember a reply you’ve made containing one of the things below.\r\n1. Bias in Levels vs. Bias in Changes\r\nSuppose you’ve got some statistic that is well-known to be biased. Perhaps it is consistently higher than “the truth” and perhaps it is consistently lower. The most obvious example I can give here is in the realm of political polling. Different pollsters use different methods - do they call landlines only? All phones? Include internet polling? Where do they advertise? How do they weight for differences in sampling across parts of the population? And so on. These decisions, intentionally or not, can lead to polls that consistently overstate support for one party and understate it for another, relative both to what other pollsters find (leading to polling “house effects”), or relative to what you see on election day.\r\nSo, if you see a poll in the United States from that one pollster who always finds higher support for the Republicans than other pollsters do, for example, you could reasonably say “hey, actual support for Republicans is probably a bit lower than that, since this pollster uses methods that tend to overstate their support.” That’s totally legit.\r\nBut what if you see a change in that value over time? Perhaps that one pollster that finds high numbers for Republicans used to say that 55% of voters were going to go Republican, but now it says that 65% are.\r\nThe critique “that increase can’t be right, because this pollster overstates the support for Republicans” no longer works! Their methods make the absolute number for support too high, sure, but unless they changed their methods recently, the ten percentage point increase still indicates an increase in support for Republicans.\r\nNow, the change itself could also be wrong (everything could be wrong), but simply knowing that the level is too high doesn’t tell you that the change is too high. Without knowing more about why that pollster gets high Republican numbers, it’s not even a reason to think that their measurement of the change is more wrong than the changes other pollsters show. If you think the change is wrong, you gotta find a reason why you think the pollster’s numbers aren’t internally consistent relative to each other, not just that they’re wrong in their absolute level. Plenty of statistics have weird absolute values but at least go up and down at the times it seems like they should go up and down!\r\nThis incorrect critique pops up anywhere you see a number that goes up or down, and for which people have issues with how it’s measured, or worse, think the books are cooked. The unemployment rate, GDP, crime rates, and so on and so on. For all of these, as long as they’re consistently-produced estimates that have a few kinks in the process (as opposed to outright lies), then just because you think the value itself is over/underestimated doesn’t mean the change in that value is over/underestimated.\r\nThis critique sometimes also pops up, although less often in my experience, in cases where people don’t think the original value is wrong, just unusual. For example, perhaps student test scores are already known to be extremely low in a certain region in your country. Then, those test scores fall even further. A critique that their now extremely extremely low scores aren’t noteworthy because we already knew that region had bad scores is, at least, missing the point.\r\n2. Demanding Infinite Detail\r\nCollecting data is hard, and expensive, and in many cases accessing data is hard and expensive, but easier to do at the aggregate level. It’s far easier, for example, to find data on the US unemployment rate than it is to find data on the unemployment rate in my neighborhood (although I know Steve’s been without a job a few months…).\r\nBecause of this, you’ll often see data posted on a more aggregate level than would be ideal. Sometimes this means geographic aggregation, like seeing the US unemployment rate instead of the unemployment rate for my neighborhood. Often it’s aggregation across groups of people. Relatively few publicly-available statistics are available broken down by age, or gender, or race, and far fewer than that are broken down by all the different cross-tabulations of age and gender and race and education level and ancestral nationality and religion and and and…\r\nA common critique I see is that an aggregate statistic is wrong or useless because it does not have a multitude of breakdowns into fractal levels of increasing detail.\r\nNow, sometimes, a statistic is indeed useless because it’s at the wrong level of detail. If the hot topic of the day is whether immigration is affecting wages in Macon, Georgia, then a graph of wages going up in Georgia overall isn’t really going to say much. “The data would be too hard to get” may be true but it doesn’t make the original analysis right.\r\nBut the problem comes when the reverse argument is made. This is where someone has a graph showing wages going up in Georgia overall, and someone comments “Useless. Wages are going down in Macon. Why bother posting this without breaking out each city individually?”\r\nInformation for each individual city might be interesting to have. But often we don’t have it, and a demand for increasingly precise levels of detail can just be an excuse to ignore an overall trend. Further, one disappointing fact about statistics is that sample size really matters. If we did have data broken down into minute subsamples, the results for each of those subsamples would be worse. Noisier, with misleading results popping up in a few of the subsamples that distract from the more accurate overall picture. Often an aggregate statistic is simply a higher-quality statistic, even if it’s a less-detailed one. This is especially true with noisy and fast-moving data, or when the subsamples would be very small.\r\nThis entire brand of critique I see a lot in regards to COVID data. Do masks work? How effective are the vaccines? Whenever someone has data that can help with an answer, the comments are flooded with people saying that there’s no point looking at the data if it doesn’t break things out by age, or by eight different comorbidities, or political affiliation, or occupation, and so on and so on. Yes, maybe those things would be nice, and they’d let us answer a different question than the aggregate data would, but the aggregate data often does answer an interesting question - sure, maybe not the one you want, but it’s probably quite useful in its own right. Skipping the breakdown doesn’t make it wrong, and there’s a good chance it doesn’t make it useless either.1\r\n3. Asking for Double-Counting\r\nA close cousin of the demand for infinite detail (#2 above) is the use of a subgroup to try to disprove a statistic. This, in effect, asks for the critique to be double-counted. I think of this commonly as coming in one of two flavors: the use of subgroups, and the use of caveats.\r\nSticking with COVID for subgroups, for example, suppose someone has a graph showing that COVID case rates are declining in Africa. Someone else might say “that can’t be right, cases are increasing in Egypt.” It might well be true that cases are increasing in Egypt. But that doesn’t mean cases aren’t declining in Africa, it just means that the increase in Egypt isn’t enough to offset the aggregate decline in the rest of the continent. Someone taking the critique further might say that the decrease in African cases should be adjusted to account for the Egyptian increase, and might even point out that doing so might lead the overall African trend to be upwards. This misses that the aggregate African trend already included Egypt, and so this adjustment would double-count Egypt.\r\nwe see another example of this with the recent debate over inflation. I’ve seen quite a lot of claims that inflation in the US must be higher than is being recorded because meat, or cars, are going up in price more quickly than the official inflation rate. I also see plenty of claims that inflation isn’t as bad as is being recorded because many different services are rising in price more slowly than the official inflation rate. Both of these miss the point that, while there are definitely valid criticisms of the way inflation is measured, inflation isn’t supposed to match the price increases for meat, or cars, or services individually, it’s supposed to record the price increase for all of them at once. If each of the individual price increases matched the aggregate statistic, that would be genuinely surprising, not a sign that it was working properly.\r\nTaking this critique to the micro-level we have good ol’ anecdote vs. data. Just because you lost your job last week doesn’t mean that unemployment is rising, and just because your grandma smoked and lived to 104 doesn’t mean smoking’s not harmful. But you already knew this one, I’d imagine.\r\n4. Misunderstanding Control Variables\r\n“Controlling for” or “adjusting for” a variable is a deceptively tricky thing. The purpose of control variables is this: you can see the relationship between two things in the data, for example drinking more water and better overall health. You think these things might be related for several different reasons: (1) perhaps lots of water does make you healthier, or (2) perhaps athletes are more likely to drink lots of water, and athletes are just healthy anyway. The purpose of a control variable is to try to exclude one of those explanations. Looking at the relationship between water and health by itself could be either of those explanations, but looking at the relationship between water and health while controlling for being an athlete should leave you with just the “water makes you healthier” explanation of why they’re related (if those are actually the only two possible explanations, which seems iffy).\r\nHow can this be misapplied in critique? One common way is to criticize a result for not controlling for stuff even when it isn’t trying to pin down a specific explanation. Controls are about excluding certain reasons why we see something in the data. But if we don’t care why, we just care that we see it in the data, then we probably don’t want to control for anything. Climate change is one place where the distinction can be clear. One critique I often see from skeptics of climate change is that it doesn’t account for things like CO2 releases from volcanic eruptions. This critique makes conceptual sense when you’re talking about why temperatures are rising - if you want to pin down humans as the reason for warming, you want to control away the other potential reasons for a rise in temperature over time, like volcanic eruptions (surely the climate scientists have never thought to try this). But it makes no sense when I see it used as an argument as to why evidence of increasing temperatures is itself wrong. If temperatures are increasing, temperatures are increasing, regardless of what caused it.\r\nAnother misapplication of control variables in critiques is in demanding that the original analysis control away the actual thing we’re interested in. It’s like if a bitter fan of a losing sports team argues the best team wouldn’t be so great without their star players. Well, yeah… the star players are what make the good team into the good team. Doesn’t mean much to say the team would be worse without them. In online political debates this, generically, often takes the form of “policy X doesn’t really work, it only improved Y because it improved Z! Controlling for Z makes it obvious X was useless.” Sounds to me like policy X worked just fine - it improved Y by improving Z!\r\nAnother place this pops up is in the extremely online debate over the discriminatory gender wage gap. Should you look for the wage gap only within the same occupation (i.e. “controlling for occupation”)? One common explanation given for why there’s a discriminatory wage gap is that women are kept out of higher-paying jobs. If you control for occupation, you’re excluding that explanation by saying it doesn’t count as part of the discriminatory gap, although it should. So clearly we shouldn’t control for occupation in that case, then. Well… another explanation often given for the wage gap is that men and women prefer different occupations. If we don’t control for occupation, we’re saying that those choices are a part of the discriminatory gap, when perhaps they are not. Hmm, doesn’t seem to work either way. The question of how big the discriminatory wage gap is possible (and if you’re interested a pretty good place to start is with Claudia Goldin) but it requires some tools beyond just controlling or not controlling for a variable. Turns out the statistical approach to answer certain questions has to go beyond what can be contained within a Twitter yelling match.\r\n5. Correlation Can’t be Causation\r\nAs we are all aware, “correlation is not causation.” Except, of course, that sometimes it is. If I jump off a building, there is a very strong negative correlation between the time since I jumped and my distance from the Earth, and I’m pretty sure it’s due to the mass of the Earth causing me to accelerate towards it. I can make this claim confidently even without randomizing myself into different jumping-off-a-building scenarios. More appropriately, the term should be “only some correlations are causal.”\r\nBeing skeptical of any statistical analysis that claims to have a causal interpretation is probably a good idea, especially if the basis of it is just some graph rather than anything more careful. Establishing a causal relationship is pretty hard, in fact I wrote a whole book about it, and too often the jump from correlational data to causal claim operates under the rigorous set of statistical procedures known as “wishful thinking.”\r\nIt may be the high hit rate of this critique that adds to it being applied where it shouldn’t. There are three main cases of this. The first I already covered in Misunderstanding Control Variables (#4): if the thing you’re critiquing isn’t trying to claim an explanation why we see what we see (causal claims are almost all forms of “why” statements), then pointing out that it’s a mere correlation is… well, they know that already.\r\nThe second case is when the causal relationship is super-duper obvious. My jumping off a building example is one case of this. In some highly convenient real-world scenarios, too, there’s really only one feasible explanation of why something happened. If you’re going to claim that a causal interpretation of the data is wrong, it really helps to actually have a realistic alternative way of explaining what’s going on. If you don’t have anything, then “just a correlation” might actually be “just a causal correlation.”\r\nThe third case is when the subject of discussion is an academic study, where the study itself is making a causal claim. Now, I’m not going to pretend that there aren’t plenty of academic studies making causal claims that are pretty weak. However, I also see plenty of critiques of causal claims from academic studies be dismissed out of hand because causal claims aren’t possible, or are only possible with randomized experiments (not true; again, I wrote a whole book about it). There are ways to deal with these problems, and it’s a good idea to at least check whether the study did deal with those problems. Often, the critiquer has thought of a pretty obvious reason that the correlation would be noncausal, except that it turns out the researchers also thought of the same thing and accounted for it.\r\n6. Giving No Slack Whatsoever\r\nWe tend to me less critical of evidence that confirms what we already know, and want to hear, than evidence that would require us to shift our opinions. I am far from the first person to notice this. This phenomenon is by no means limited only to people talking about statistics, but statistics makes it rather unfortunate.\r\nWhy? Because statistical evidence is imperfect, and statistical evidence varies.\r\nStatistical analysis is always imperfect and incomplete and, to some degree, wrong. Doing analysis requires making assumptions about where the data came from and how it fits together with the real world. Plus, those assumptions aren’t really ever going to be true. We’re just aiming to make assumptions that are close enough to accurate that they don’t mess us up. And people will disagree about what those assumptions should be.\r\nAnd statistical evidence varies. We’re going to get different results from sample to sample, and from place to place. Even if we did an analysis that was perfect, if we do it again enough times in enough different places, we’ll eventually find a result that contradicts what we started with.\r\nStatistical analysis is about getting close enough to the truth to be useful, not actually true, and getting results that are consistent enough to point in the right direction, not always the same every time.\r\nBecause of this, it’s trivial to find some flaw in a statistical analysis. And it’s often not difficult, at least if a topic is well-studied enough, to find some piece of contradictory evidence (even easier if you allow that contradictory evidence to be itself very weak or poorly done).\r\nMoreso than other forms of evidence, it’s a good idea to ask of statistical evidence not just whether there is a flaw, but whether that flaw is important enough to make the evidence not worthwhile. Perfect evidence is never coming along. If you’re in the habit of taking any flaw in, or counterexample of, a piece of evidence you dislike as a reason to completely throw it out, you’re going to be able to do so. But do this enough times and you might find yourself throwing out a pretty strong mass of evidence, one piece at a time. This is doubly true if you’re making excuses for similarly-trivial flaws in analysis you like.\r\nThis doesn’t mean you have to accept results uncritically. But it does mean that, more than other forms of evidence, you want to take statistical evidence as a whole, not just one piece at a time. Your one counterexample might weigh up against a single example, but if it turns out that it’s more like one mildly flawed counterexample to ten mildly flawed examples, that should mean something too.\r\n\r\nPlus, doing these breakdowns is often not possible with the available data, and would be a lot of work if it was, so it’s not super fair to demand someone do it.↩︎\r\n",
    "preview": "posts/2022-01-23-overdebunked/howtolie.png",
    "last_modified": "2022-01-26T10:38:58-08:00",
    "input_file": "overdebunked.knit.md",
    "preview_width": 256,
    "preview_height": 387
  },
  {
    "path": "posts/2021-11-19-consulting-and-freelancing-in-academia/",
    "title": "Consulting and Freelancing in Academia",
    "description": "A guide to getting started and doing well at consulting and freelancing, and avoiding some speed bumps. This is aimed squarely at people in academia who are good at working with data.",
    "author": [
      {
        "name": "Nick Huntington-Klein",
        "url": {
          "https://www.nickchk.com": {}
        }
      }
    ],
    "date": "2021-11-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWhy?\r\nHow Should You Prepare?\r\nWhere Can You Find Jobs?\r\nGetting Started\r\nHow Can You Deal with Clients?\r\nWhich Clients Should You Avoid?\r\nSuccessfully Working with Clients\r\n\r\nManaging Your Time\r\nDon’t Forget Taxes\r\nAnd So\r\n\r\nI am an academic. I am very well suited to life in academia. I love every part of it, including teaching, and have very little interest in leaving it. This is true of my attitude and interests, and remains true despite the fact that I am unlikely to ever be published in a top journal in my field, and also that I have maintained, while in academia, a pretty lively string of consulting and freelancing assignments.\r\nIn this post I will share with you some guidelines and tips for pursuing consulting and freelancing work while inside of academia, either as a grad student or as a professor. This will be targeted at people with a skillset similar to myself: people with lots of technical and analytic skill when it comes to working with data. It will also heavily suggest doing things in the exact way that I did them, and in the United States, because that’s what I know. I will not pretend that this is the only way to do things. But it is one way.\r\nI have decided that the most appropriate tone for a post like this is “world-weary” and I’ll do what I can to rise to this standard.\r\nWhy?\r\nYou’re busy as heck. Why should you consider doing any sort of consulting?\r\nIt pays you money. I have no complaints about my salary as a professor, not one, not ever. I hear professors complaining about how little they get paid and my mind explodes. I’d get it from, like, English professors maybe, but I’m talking business school people. I come from a background where the amount of money I make as a professor is stupid fantasy money. Just ridiculous. I look at the US income distribution and go “welp, I’m rich now I guess.” In my first job out of grad school my annual salary was more than my parents at their highest-earning ever, combined. But professor salaries are indeed quite low compared to what that same skillset can earn you in the private sector, which to me is an even stupider, more fantastic amount of fantasy money. Many professors I know have a spouse with a similarly-remunerative job. Assortative mating. I support a three (soon to be four) person family in what I at least imagine to be an upper-middle-class lifestyle on a single income and without, like, a trust fund or whatever. Consulting makes that possible. If you know how to work with data, your time is extraordinarily valuable, more than I can reasonably understand but apparently the market does. So get paid for it, at least sometimes.\r\nYour academic job will usually be cool with it. Obviously check with them, etc., and take stuff into consideration like “can I use my school laptop for this work?” and “will it be a problem if I respond to client messages while on the university Wi-Fi?” and “is my proprietary software like Stata, Matlab, or Tableau licensed for commercial use or just academic use?” but there’s a good chance nobody will get mad at you for taking consulting jobs.\r\nIt provides interesting projects, occasionally. Most of the projects you’ll work on as a freelancer will be pretty boring and basic. But occasionally you’ll get forced into an arena you’re not familiar with and will learn some cool new stuff. I’ve certainly been exposed to all kinds of interesting real-world detail in areas I wouldn’t have bothered to look in before. Maybe the most interesting part to you, the academic isn’t the same as the most interesting part to the client, but who cares, you’re enjoying yourself. On extremely rare occasion your project will end up in a publication for you, too (I’ve done it once, with more on the way).\r\nIt develops your skills. Projects you get as a freelancer will often require you to use tools that are more common in business than in academia. Python, SQL, Excel, etc. It will also require you to think in different ways about what the goal of your work is (clean data, actionable takeaways, easy workflows rather than publishable insights or pristine research design). Sometimes this can lead to you being better-equipped for the next round of academic tech updates (who knows what language will be en vogue in 10 years?). Sometimes it will expose you to entire new fields (for example, the Event Studies chapter of my textbook was heavily informed by consulting jobs I’d been given in finance, a field I otherwise have very little interest in). This will also leave you more equipped for, well, working in the private sector (and better-connected! Two different clients have offered me full-time private sector jobs). Maybe that’s what you want - if you’re in grad school and are thinking about the private sector this is one way to build up some experience short of taking an actual job concurrent with your studies. Or maybe it’s not what you want but you think you might end up there anyway. I’m in the US, and I think there’s a non-ignorable chance that within the span of my career either the federal government, and maybe many states, will just sorta defund colleges and/or student aid massively, or that some new form of career prep will outcompete universities and demand will plummet. Where will that leave regional-university non-superstars like me? Like me? Dunno. Me? I’ll be well-equipped to jump ship to some sort of tech company. I will miss academia, and the stage of society that supported it so much, dearly if it comes to that, but at no point will I miss eating. Or a mortgage payment for that matter.\r\nIt provides immediate purpose. I rarely know if my academic research is having any real effect on the world at all. Even when it directly affects some sort of policy change I don’t really know what change is being affected. But consulting work? Sure, maybe the end goal isn’t quite as noble as improving humanity’s stock of knowledge. But it improves someone’s knowledge about something, and you can see it work. That’s nice. Not to mention, my contracting work has led me to projects with actual academic-style noble-purpose outcomes. I’ve ended up in a number of projects that improved policy for nonprofits. I contributed to a governmental COVID task force, which is a project I got through a for-profit client, not my academic work. I’ve produced plenty of education materials on a contracting basis (in addition to the ones I make for free as public goods) and you can see those get used immediately.\r\nThat said, yes, this will eat into whatever free time you have, and you’ll want to make sure you’re doing well at your main academic job as a prerequisite for spending time on other stuff. Reviews at my academic job suggest I’m not skimping there. If that started to slip I’d reevaluate.\r\nIn my case, it’s definitely true that I work all the time and have had to largely excise everything but work and family from my life. But I’m okay with that. I say that not as a brag or judgment on how much people should work, but just an honest statement that I am pretty happy with a work/life balance the internet tells me I should not be happy with. I enjoy working. If you enjoy working less than I do then you should work less than I do. Consulting, conveniently, does let you scale how much work you’re doing, as long as you’re willing to say no when you have too much.\r\nHow Should You Prepare?\r\nDo you know how to clean data from your academic experience?\r\nDo you know how to work with data and do the best analysis you can based on your academic experience?\r\nCan you explain your results in an extremely simplified way to someone who has never heard of a regression?\r\nAre you ready to accept that not all analyses need to be as careful as those heading for an academic publication, nor do they have the same goal as an academic publication, but that you need to find a way to be rigorous regardless?\r\nIf so, you’re good to go, at least conceptually. Technically you’ll be doing yourself a favor if you’re skilled in common private sector languages like Python and the various flavors of SQL (or, more importantly, can pick up new tools and languages on the fly). Or learning to do stuff without relying on fancy models. Or using Excel.\r\nThere are some parts of freelancing that have to do with changing the mindset you have around the work that you do (see the final point above). This is the kind of thing you’ll just have to get through practice, though, so I won’t bother trying to preview it.\r\nWhere Can You Find Jobs?\r\nI find jobs largely on upWork. There are other places with freelance job listings, too. If you can manage to get yourself hired directly by a company that needs someone on an ongoing basis and will pay you a decent rate, that’s fantastic. I have, on occasion, built contacts through upWork that later turned into far more lucrative contracts direct with a company that has deep pockets.\r\nYou might wonder about the value of something like upWork given that they take a relatively hefty cut (20% for the first $500 with a client, 10% afterwards). But it’s worth it. The platform provides a lot of clients. More than that, they make sure you get paid. If you work with someone one-on-one, who knows what happens. Wait a few months to get paid, never get paid, spend a lot of time chasing payment, etc.. upWork forces them to pay once you’ve agreed to something, I’ve had literally two contracts ever go south out of hundreds (and one of those went south in my favor, so hey!), and that’s absolutely worth 10% on its own.\r\nupWork isn’t where I got started though, nor is it the only place I get work. I started out my freelancing focusing on education rather than data work. I did tutoring through Tutor.com and Chegg Tutors, I wrote a zillion practice questions for Albert.io, I wrote practice questions for textbook publishers. I generally don’t advise this path for people situated in academia. This work is rote, time intensive, and poorly paid relative to working with data. Not to mention exposure to actual students in a freelance online-education setting means you’ll spend the majority of your time dealing with people begging for you to just do their homework for them instead of getting paid (or giving in to those requests, and I hope you have more dignity than that). If you’re a grad student and don’t think you have the data chops yet, then sure. This stuff is at least low-commitment and can earn you a few bucks.\r\nBetter opportunities outside of a job board like upWork often require connections of some sort, or being known for something. I’ve had a number of great opportunities come from being referred by a client I met on upWork to another client not on upWork. I’ve gotten hookups from friends who have gone into the private sector. I’ve also had companies email me to offer work. When you’re a professor it happens! You might also come in contact with middlemen consulting agencies who link up freelancers with bigger companies. I’ve worked with a few such agencies, most of which hired me the first time off of upWork. All these experiences have been good. They have access to bigger clients that you’d be unlikely to find on your own. Good pay and longer-term projects.\r\nSome of the most stressful and frustrating, but financially rewarding, work comes in the form of legal-case consulting. Remember the stupid fantasy money I mentioned in data work? Well, being an expert in a legal case, or just offering commentary and advice to one, is super-duper stupid fantasy money. If you’ve ever been on a jury for a trial with experts, you know lawyers love to ask the opposing experts what they get paid, and for good reason - what juror wants to listen to someone getting paid ludicrous amounts to be there? I’ve done two legal cases, one which came to me through upWork and one which was an email request, and each time charged several hundred dollars an hour. This is low for what this usually pays.\r\nYou do have to put in the work, though. Lawyers expect extremely fast turnarounds, there are often restrictions on the scope of what questions you can answer or what methods you can use to answer them (if you’ve ever read an expert opinion for a legal case from an academic you like and thought “wow, this is not as good as their usual work,” this is one reason why), and you might have to give deposition or testimony which can be a little terrifying. Plus, while this isn’t something I’ve experienced myself, you might end up in a situation where the lawyers really want you to find a way to make their side right, rather than providing your actual best expertise, also then also put your name on it. The legal-case work I’ve done has all been in the form of rebuttal to other experts which makes some parts of this easier but not all of it.\r\nOn a much smaller scale, I will, as someone with a mildly public persona via Twitter and YouTube, occasionally get requests straight to my email from individuals. Many of these are requests to help in general without pay (which I’m happy to oblige, up to a point), but others are offers of work. These might be worth it if you happen to have the time, but I don’t think I’ve ever had one blossom into a truly worthwhile contract (see the note about “small contracts” below).\r\nSo how do you get work, in sum? At least starting out, if you don’t have any connections your best bets are going to be a job board of some sort, or just waiting for the right opportunity to hit your inbox. It really does happen!\r\nGetting Started\r\nWhen you are starting out, you’re probably going to be starting small unless you have some sort of hookup to a major project. This means building a good profile on a job board like upWork and accepting small jobs that you only expect to pay a few hundred dollars in total. At this point you can start working pretty quickly, but it’s unlikely that you’ll be making this whole venture really worthwhile right away, given the opportunity cost of your time. But you’re building a profile with some hopefully satisfied clients, and getting used to how these consulting jobs work.\r\nAt this point you’re giving yourself a crash course in the parts of consulting that your academic and data skills don’t cover. You’re building negotiation skills and client-management skills. You’ll want at least a little of that under your belt before taking any real big projects. Try to focus on taking projects where your skills are handy (i.e. sure you could proofread a college kid’s essay for a few bucks but that’s not really teaching you much in terms of building to something bigger), and that you can turn around quickly. Small projects with well-defined goals are great here. By doing these you’ll be able to build a set of positive reviews (if you’re on a job site) and a general knowledge of how this work operates without getting bogged down by any one project. You’ll also get to know your own work habits and speed better. How long will that project really take you? Do you really have time to do it? You’ll develop a much better internal sense of this.\r\nOnce you feel like you’ve got a handle on it you can go for bigger projects, and be more likely to land them because of your track record and improved ability to sell yourself to this audience. Bigger projects are generally better once you know how to handle them (see How Can You Deal with Clients below).\r\nHow much should you charge starting out? Don’t undersell yourself. If you’re on a job board, check what other people charge. You may be surprised. If you’re just playing around and trying to get a few small contracts under your belt, sure, charge fifty bucks an hour or something. But once you’re actually trying to earn money, or being asked to take on something that will actually compete for time with the rest of your busy schedule, you should charge what the market will bear! And the market will bear a fair amount. $100/hour is a pretty low-end amount for an academic with data skills. I charge different clients between $90 (for long-time clients with consistent low-difficulty work) to $150/hour (for new clients) these days, but I already get more offers than I can handle and so that $150 could probably be higher. Keep in mind that clients will often take too-low wages as an indicator of poor quality. Maybe it’s just the upWork algorithm but whenever I raise my price I get flooded with new job requests. A high hourly rate will also deter people who do not actually want to pay you (see How Do You Deal with Clients? below).\r\nWhatever hourly rate you land on, if you come to a project that pays for the final deliverable rather than hourly, you should think of that project in terms of how much time it will take you and charge based based on your hourly rate, plus a markup since you’re taking the risk of the project requiring more time than you expect. You can even be explicit about it. I will often tell people asking for project-based payment “My hourly rate is $\\(X\\). I expect this will take \\(Y\\) hours. So I will charge \\(X*(Y+Z)\\) to cover the possibility that it will take longer. Or if you prefer, we can do an hourly rate, and maybe you won’t need to pay for \\(Z\\).”\r\nOne last thing - in the “figure this whole consulting thing out and get established” phase, since the projects you’re taking are smaller you’re likely to stumble upon some odder or more fun ones. Go for it, this is your time. You can definitely afford to cut your rate for someone offering you an evening of entertainment. This was the stage of my freelancing when I was definitely happy to write assignment guides or check someone’s answers on their game theory homework, because I think game theory homework is fun and I miss being assigned it. This is the person you’ve chosen to read an article by and take advice from. (Note: when a student contacts you offering money to solve their homework or write their essay for them, obviously not something an academic should be doing, instead offer to write them an assignment guide, which includes no solutions but is a set of tips and nudges in the right direction to help them solve the assignment themselves. A surprising number of them will take you up on it).\r\nHow Can You Deal with Clients?\r\nManaging (and selecting) clients is a task that is really very different from something a typical academic is going to run into. Their expectations, desires, and ways in which they will take advantage of you are entirely different from the expectations, desires, and advantage-taking-ways of an advisor, boss, or purse-string-holder in academia.\r\nWhich Clients Should You Avoid?\r\nAvoid any clients who are incapable of telling you what they want you to do. They will decide, after you have done it, that they actually wanted something else.\r\nAvoid anyone who wants to spend a lot of time negotiating the price with you or asks for a lot of discounts, especially if the project isn’t that big anyway. Negotiating is fine but if it’s more than one round of back-and-forth and the amounts you’re negotiating over aren’t enormous, start looking for the exit. Avoid these people for four reasons: (1) you don’t get paid for that negotiating time and headache, (2) after they hire you they will likely be trying to find ways to stretch their dollar even further - asking for free extra little tasks, asking for partial refunds on pieces of the project that didn’t go as planned, and (3) your time is valuable and scarce and thus expensive. If they don’t want to pay your rate they they don’t have to, but you aren’t obliged to work for them. You can literally tell people who ask for big discounts that sorry, you know you’re expensive, but that’s how it is, there’s probably someone else out there cheaper. I do that. People get it. Finally, (4) I can only speculate as to why, but every client who ever ended up really mistreating me - yelling at me, belittling me, trying to jump the bill, gaslighting me about what work I’d agreed to do, etc. - every single one started out as an exhaustive negotiation over nickels and dimes.\r\nAvoid anyone who wants to have a whole lot of scheduled phone or video calls with you to check up. Video calls aren’t inherently a problem but I mean people who want constant progress check-ins in the form of calls, more than would actually be necessary for the project. Lots of expected calls should be a dealbreaker if they’re not offering to pay for your time on the call. Run. If the contract being offered is such that you get paid for your call time (even if payment isn’t hourly-rate, the calls should be listed as a part of the work you’re getting paid for), then it’s not a dealbreaker, but it’s not great either. You’re a busy academic. Consulting fits into the cracks in your schedule. I do the majority of my consulting work at night after the family is in bed. As soon as consulting work has to find a place on your regular work-hours schedule, it is consuming a much more valuable portion of your day.\r\nAvoid anyone who wants you to do bad work, at least most of the time. You’ll get plenty of requests from people who have clearly decided what they want the data to show, and they want you to squeeze and squash the data until that happens. Or they want you to specifically use a method you know is garbage. Sometimes you can talk these people out of it - you’re the expert after all. Explain why what they’re asking you to do is a bad idea. But sometimes they still want it anyway. If it seems like the only one getting hurt by the bad analysis is them, well, it’s their grave, it’s up to you at that point if you want to help them in. But if the results are going to end up “informing” the public or affecting people, do the right thing and back out!\r\nAnd finally, as time goes on, shift towards avoiding clients with small one-off projects more and more. There’s nothing wrong with these, but new projects require setup, price negotiation, figuring out the goals, learning the ropes, etc.. You’ll be using your time more effectively, and also have much more predictable responsibilities and income, with bigger projects. These bigger projects can run from month-long projects that pay 1-2 thousand dollars to multi-year engagements as the resident data consultant for a small company. So once you have enough big projects to fill your clock, avoid taking on small ones.\r\nSuccessfully Working with Clients\r\nThere are lots of things to consider when handling clients - being kind and prompt, returning work quickly, not letting them take advantage of you, and so on. But the most difficult and important part about working with clients, and what I’ll focus on here, is handling their expectations. This includes what their expectations are of you and also what their expectations and desires are for the work you will give them. The reason this is so difficult is that private clients often have very different expectations and desires than you’ve been trained to think about in academia. So there’s a minor culture shock.\r\nThat’s your first goal, then. Figure out what it is they actually want. As a general rule, private clients are more interested in results than they are in methods. What are the takeaways? A reasonable data-driven hint as to what they should do is usually way more valuable to them than a caveat-filled conclusion that you’re absolutely certain has been supported by the data and buttressed against any possible criticisms.\r\nWith that in mind, you get a little bit of freedom to not be quite as rigorous as you would be in an academic setting. But this also means that you’ll be doing a little dance of figuring out exactly where the line is between “result that’s not quite as robust as something I’d try to publish in an academic journal” and “result I don’t believe at all.”\r\nThis leads us back to handling client expectations. Letting them know what you can and can’t do with the data that’s been provided is key, and will help keep them from expecting more of the data than can actually be achieved.\r\nDoing all of this properly requires understanding how clients think about the kind of work you do, a topic about which I had a Twitter thread long ago. Many private clients think of statistical analysis as a product they buy in a store that solves a problem for them, rather than as a tool for unconvering some sort of truth.\r\nThis means that, at least initially, many of your clients will be expecting you to provide results that justifies a particular already-made choice, or looks good for them in some way, or provides a very clear and distinct answer to a super-important question.\r\nObviously, results from data do not always fall this way. This can lead to problems. Clients will ask for a zillion variations until some result implies what they want (whether or not the result is mush by that point), they’ll ask for a refund, or, worse, they’ll string you along with revisions until something pops out that they like.\r\nIn my experience, the best salve here is to simply be honest. Remind clients that if they already had a conclusion in mind there’d be no need for data analysis. Tell them that, yes, in your expert opinion this really is what the data says. This approach doesn’t always work, but it works sometimes, and I’ve yet to find a better alternative.\r\nLetting clients know what they will, and will not, get from you is a pretty good idea in general. Yes, you can build a predictive model. No, you can’t guarantee it will boost sales.\r\nThere are a bunch of other skills that you’ll learn in the process of doing consulting work that will help you manage clients better, but I’ve been thinking through them and for the life of me I can’t think of any that I could adequately inform you about through text. So you’ll just have to work past those issues yourself.\r\none thing I have left out, though, is managing academic clients. You’ll actually get a lot of these, or at least I do. This makes sense - you’re an academic. They need academic skills. Academic clients range from students wanting help with their assignments up to other academics basically hiring a postdoc but with lower commitment.\r\nAcademic clients can be great clients! But some things to keep in mind:\r\nIf the client’s a student, make extremely clear up front that you are helping them but aren’t doing their assignment for them.\r\nIf the client’s a professor, treat them like you treated whoever you may have RA’d for in grad school (if you did that) and you’ll be golden. This isn’t your project and you aren’t a coauthor, do as you’re told. That said, don’t agree to coauthor with them unless you like the project and think you’ve contributed enough. I’ve been offered authorship on an astonishing number of papers that I did some very basic RA-level work on. I’ve always said no. Depending on your affiliation, this can sometimes be a very-transparent play to get a bad article in a bad journal instead of a very-bad journal. Not worth it coming up on a Google search of your name.\r\nWhoever the client is, there’s sort of an expectation that, as an academic, this is the sort of thing you’d normally contribute to without pay. So the relationship can sometimes become less than business-like. But you can’t help every struggling grad student or assistant professor in the world. Keep it professional, clients need to pay for your time.\r\nThat all said, academic clients can be great, and I will in many cases take lower payments for them. One aspect of freelancing I always enjoy is when grad students with terrible advisors come to me for help. I’ve never worked at a university with PhD students, but I’ve always wanted to be a dissertation advisor. This fills that lil’ gap in my heart. I don’t know where these grad students come up with the money to pay an advisor on an hourly basis (even at a discounted rate), but I’ve been the de-facto dissertation advisor to five students, four of whom ended up with PhDs (plus, served on the actual committee for a sixth, who for the record did not have an awful advisor and brought me in for coauthor-level work on their stats). Loved every minute of it.\r\nAlso, five of you out there are just truly awful PhD advisors, and have your name as dissertation advisor on a student’s CV when you didn’t do jack, I did. I do know who you are.\r\nManaging Your Time\r\nYou’re a busy person who already has a job. Consulting isn’t going to work for you if you can’t manage your time properly so it doesn’t get in the way of your main job (or the rest of your life for that matter).\r\nWhen thinking about how to manage your time, there are three big things to keep in mind. The first is in taking on the right amount of work, the second is in taking the kind of work that gives you flexible scheduling, and the third is in avoiding project creep.\r\nTaking on the right amount of work. You should have a decent idea about the amount of time you have available to dedicate to consulting. Then, don’t take on more work than will fill that time. This would be easy, except of course that hitting the sweet spot of how much work you’ll take on can be very difficult.\r\nThis is not the most original of advice, but it’s a good idea to look over a project carefully before committing to it, thinking about how long you expect it to take you, and then realizing that it will almost certainly take you longer than that. Pad your expected contribution, because you have no idea where surprises will lurk for you! This is important both for managing your time and in quoting prices to clients - anyone asking for an expected amount of time something will take you (if they’re paying hourly) or a quote (for project-based payment) should get a number that is at the very least 20% above the time you actually think it will take you.\r\nTaking on the right amount of work also means favoring projects where you have a better idea of where the endpoint is. Projects that change and expand as time goes on make it very difficult to manage your time (see avoiding project creep below).\r\nPursuing flexible scheduling. The easiest way to fit consulting work into a life that already has an academic job in it is to do work that can be done any time. This means avoiding work that requires lots of scheduled calls or that otherwise have to be done during business hours.\r\nThis also means limiting the number of projects you have with very tight deadlines. Outside of grant-writing, deadlines for academic work are often suggestions - conference submission dates get pushed back, referee reports languish, the edit you told your coauthor would be done next week is, uh, well. There are plenty of hard deadlines in academia, university admin deadlines and so on, but not so much for academic work. If you need more time to do it right, then take more time until it’s done right. Private clients do not usually see things this way. If there’s a deadline, you need to meet it.\r\nTight deadlines do not give you flexibility, though. If I have a week to do a four-hour project, I can do 45 minutes a night and be done easy. If I have until tomorrow, well that’s my day. Flexibility out the window!\r\nSo unless it’s a really good offer (like, say, a legal consulting gig), don’t take projects that you’ll have to crunch to finish. And if you have some control over the deadline, don’t feel bad about pushing it out. Remember the padding for how many hours it will take you to do the project? Well, take that padded number and think about how many days it will take you to cobble together that much worktime. Then pad that.\r\nIt does not feel good to have to push back working on a research project you’re excited about because you gave a client a too-short deadline. So don’t put yourself in that situation!\r\nAvoiding project creep. Certain projects have ways of expanding as they go along. Sometimes this is fine. Projects that go well get sequels, so to speak, and if it’s work you like, and you can plan ahead for those sequels, then great! That’s just a return customer. But some projects expand in bad ways that can leave you frustrated and eternally unable to bid farewell to the project. Unfortunately it can be tricky to catch these up front a lot of the time, but there are warning signs.\r\nOne major warning sign is if you are going to be working with data that you can tell is super noisy or just isn’t very good, or are being asked to use bad methods. Data analysis works to some degree on a garbage-in-garbage-out principle. In my experience, clients are very willing to push back on “garbage-out,” and will tell you that results aren’t plausible and need to be fixed (good instincts born of deep contextual knowledge!), but they will often fail to accept “garbage-in” as an explanation (bad instincts born of a lack of data-analytic intuition). Thus begins the project creep where you are asked to try an ever-expanding list of alternate methods so as to find the one magic configuration that produces a believable result. All the while the entire project annoys you because you know the work you’re doing is very, very bad. Plus, if you agreed to project-based payment, you’ll quickly hit the point where the project is no longer a good deal for you, but the marginal cost of just one more variant is low enough that you keep just hoping it will end and they’ll pay up. Worst-case scenario is they try not to pay because they don’t like the result.\r\nYou can avoid some of this by building a maximum number of revisions into your project-based contract. This is harder to squirm out of if you’re on an hourly contract though. What you should really do is do a very careful assessment of whether the data is likely to produce anything believable, and back out if it’s not up to the task. You can try saying something to the client ahead of time like “hey I’ll run this if you want. It will probably be garbage though,” but almost every time I’ve done that they’ve said to go ahead anyway and the same cycle has occurred.\r\nOn bigger projects where you’re a consultant working with a team, you can avoid some of this project creep by being sure to talk extensively about the data directly with the person who knows the most about it. The number of projects I’ve had to redo because the middleman between the data person and me didn’t explain something properly and I had to redo everything at the last second!\r\nAs an aside, you will also often find it worthwhile to anticipate project creep before it happens, and prepare yourself for it. The only advice I really have to this end is to always write your code in a way that’s easy to expand upon. Private clients, just like Twitter commenters, love to look at your final result and say “neat, but how about you look at that same result in these twenty different subcategories?” At that point you’ll be kicking yourself if you haven’t already written your code in such a way that you can just loop over it to get results by subcategory.\r\nDon’t Forget Taxes\r\nThis section is completely US-centric. It seems important for this section to point out that I’m not an accountant and this is not, like, professional tax advice or anything. So, really, double-check everything I’m telling you against an official source. My only credentials are that I have been doing my own taxes a long time and know my way around this stuff pretty well.\r\nConsulting income is indeed income. If you’ve only ever worked, like, actual jobs before with W-2s and such it can be easy to forget that that one-time project you did in May that paid like $800 is also income that you have to pay taxes on. In fact, there are a few things that might change about the way you do taxes if you start doing substantial amounts of consulting. For one, you might not be paying taxes just once a year any more (so if you start getting real into consulting, don’t think you can just wait until April and tack on an amount and see what happens, you may be in for some late fees).\r\nThings to note, especially if you are doing your own taxes:\r\nYou won’t get W-2’s for your consulting work, so don’t wait around for them to come, or think that you don’t have to report the income because you didn’t get a W-2.. For bigger clients (or job boards like upWork) you’ll likely get a 1099. But even if it’s for a smaller client not on a job board, you’ll still need to report the income. So keep track of it!\r\nAs soon as you start doing consulting work. you’re a business! A sole proprietorship. That doesn’t mean you have to do the whole rigamarole of filing business taxes and registering as a business, though. For the most part, if you want you can just keep doing your regular ol’ individual-person taxes, and there are just a few additional forms to add with places to put your non-regular-job income.\r\nYour typical paycheck from a job will helpfully withhold taxes for you so you don’t need to think about it. No longer! You need to withhold your own taxes. Figure out ahead of time what your tax rate is (which includes income taxes and some other stuff I’ll talk about here), and set aside that portion of all your consulting income. Further, you need to send those withheld funds into the IRS four times a year. See Form 1040ES. If your state has income taxes it will likely have a similar process.\r\nYou’ll need to report this income on a Schedule C form (and then also Schedule 1) when doing your 1040.\r\nOn that Schedule C is a spot to report business expenses. Yep, you have business expenses now! This is not the same as your regular-ol’ 1040 itemized-or-standard deduction for personal income. Business expenses are counted directly against your business income. Make sure only to report stuff that you only use for your consulting. Buy a laptop for both your consulting work and your academic job? Not a deduction. But run an AWS server for your clients? Bought a Tableau license for your consulting? Deduct it. There are even ways to deduct part of your rent/mortgage if you have a room in your home that is only used as your office for consulting work.\r\nYou don’t just need to withhold income tax. Payroll taxes apply to consulting income, too. Plus, payroll taxes are split between employer and employee. You’re both now! So you’ll be paying both sides (although one of those sides is itself tax-deductible). Fun. File a Schedule SE (“self-employment tax”).\r\nOn the flip side, check out the qualified business income deduction! You qualify for that now (or at least I do). Sweet, eh?\r\nIf you start doing a lot of consulting and plan to keep it that way, you might consider setting up an S-corp (pass-through corporation). This can reduce your payroll tax burden (as some portion of your consulting income comes due to you as owner of the S-corp rather than to you as someone who works in the S-corp) and also changes some other stuff. You’ll have to pay to incorporate, which is a downside, and you’ll have to start filing quarterly 941 forms and also annual business taxes (including unemployment on a 940). Oh yeah and the filing months for the quarterly 941 forms don’t line up exactly with the quarterly 1040ES forms for some reason. Your taxes will get complex enough at this point that you’ll definitely want to pay an accountant to do them for you, which will likely cost a couple thousand, at least for the first year (after which point you can crib off their work if you want). I have an S-corp and I’m not yet sure if it has turned out to be a good idea. A year and a half into it and I have maybe saved enough taxes to offset the setup costs. Maybe.\r\nTax rules are very complex to figure out, but tax forms are pretty simple little homework assignments. I strongly recommend making an Excel workbook that does the math of the 1040 and all its sub-forms for you, and also has a place for you to enter all the payments you’ve received (which feeds straight into the appropriate 1040 cells). This is not very much work to set up, and once you’re done you sort of have your own lil TurboTax. Benefits include: (1) your taxes are already done when it comes time to file taxes, (2) you definitely want to keep good books now that you’re doing your own. The part of this process where you set up a spot to record all the times you get paid means you’re now keeping good books. And (3) assuming you’ve also included the 1040ES worksheet in there, which you should, and also a place to put in all your actual W-2 income with witholding and stuff, you’ll know exactly how much you need to mail in when it’s quarterly-taxes time.\r\nAnd So\r\nConsulting! Freelancing! Try it out. If you have the time, sign up for upWork and do a few jobs, see if you like it. You may get a lot of interesting and educational experiences from it, too, in addition to being paid. It doesn’t even have to become an enormous part of what you do, since you can largely set your own limits on how much you work. I’ve found it pretty rewarding in many ways, and hopefully this article can help you avoid some of the headaches I ran into.\r\nDon’t do more of it than you want to. But also don’t do less of it than you want to. Do the correct amount of it. This is useful advice.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-19T03:58:55-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-16-identification-by-heteroskedasticity/",
    "title": "Identification by Heteroskedasticity",
    "description": "A brief note on a method for how we can identify a causal effect using assumptions about heteroskedasticity instead of correlations.",
    "author": [
      {
        "name": "Nick Huntington-Klein",
        "url": {
          "https://www.nickchk.com": {}
        }
      }
    ],
    "date": "2021-11-12",
    "categories": [],
    "contents": "\r\nIdentification via Heteroskedasticity\r\nNote: after posting this on Twitter it became very apparent that I did not invent this idea, and if you want to explore it further I recommend a number of papers by Arthur Lewbel on this topic. But perhaps you will still find the intuitive explanation here useful.\r\nIn this article I will produce a sketch to demonstrate an entirely new approach to identifying a causal effect that relies on an entirely different set of assumptions from what you’d normally see. Why am I introducing this via blog rather than in an academic publication? Simply, if I put the time into developing this into an academic publication, I can tell you exactly what the reviewers would say: “okay, sure, but in what context do these different assumptions actually hold?” and I’d give a big ol’ shrug because I don’t know. I still think it’s pretty neat if only as a curiosity. And hey, if you’re out there and thinking “hold on, I do actually know a context where this can be applied!” then please do get in contact with me. I’d love to hear about it, and maybe then I’d turn this into a paper.\r\nAt this point, it’s really just an idea.\r\nThe basic idea begins with the basic way we usually identify the causal effect of \\(X\\) on \\(Y\\), which is through the use of assumptions about variables being unrelated. You almost always need one (or more) of these assumptions to be able to identify a causal effect. If you want to simply estimate the relationship between \\(X\\) and \\(Y\\) and say “this relationship describes the causal effect of \\(X\\) on \\(Y\\)” you need to make an assumption similar to “the other determinants of \\(Y\\) that are not themselves determined by \\(X\\) are all unrelated to \\(X\\)”.\r\nThis assumption is actually stronger than we need it to be. What we actually need, in a regression for example, is that variables are uncorrelated. This is not as strong a requirement as being unrelated, since “uncorrelated” only cares about linear dependence. If the graph with \\(Y\\) on the y-axis and \\(X\\) on the x-axis has the shape of a U, for example, then \\(X\\) and \\(Y\\) are clearly related, but are uncorrelated.\r\nWe’re used to assumptions about things being uncorrelated. When thinking through a causal analysis that uses regression, we constantly ask whether the mean of the treatment variable \\(X\\) is likely to be higher or lower based on the value of some variable that’s been excluded from the regression.\r\nWe think about other elements of two variables being related besides correlation in some ways too. We think about heteroskedasticity, which is the relationship between \\(X\\) and the variance of the error term (which consists of terms excluded from the regression). Heteroskedasticity isn’t important for identifying the effect, but we do need to keep it in mind when calculating standard errors.\r\nBut what if we could flip that? What if assumptions about heteroskedasticity were what we used to identify our effect, and assumptions about correlation weren’t important?\r\nThis is possible under the right assumptions!\r\nThe Basic Idea\r\nThe insight here is that heteroskedasticity allows you to pick up a causal effect (or at least its magnitude) in a sort of dose-response kind of way. Intuitively, if you make the treatment more noisy, then if there’s a causal effect, the outcome should get more noisy at the same time. We can detect the extent to which this happens by comparing variances. That’s it, that’s the idea.\r\nA Mathematical Demonstration\r\nLet’s do a demonstration. We are trying to get the effect of \\(X\\) on \\(Y\\), both continuous and mean-zero. However, there are two confounders, one binary observed confounder \\(Z\\) and one unobserved confounder \\(W\\). The data generating process is as follows:\r\n\\[ X = \\gamma_z Z + \\gamma_wW + \\nu \\]\r\n\\[ Y = \\beta_x X + \\beta_z Z + \\gamma_wW + \\varepsilon \\]\r\n\\[ \\nu \\sim N(0, \\sigma^2_X \\equiv 1 + Z\\theta_{xz} + W\\theta_{xw}), \\varepsilon \\sim N(0, \\sigma^2_\\varepsilon) \\]\r\nIf we use a method that relies on \\(E(XW)=E(X)E(W)\\), like regressing \\(Y\\) on \\(X\\) and \\(Z\\) and looking at the coefficient on \\(X\\), we’ll be biased, since \\(X\\) and \\(W\\) are correlated.\r\nBut what if, instead, we look separately at the \\(Z = 0\\) and \\(Z = 1\\) groups. Then, in those groups, we examine the variance of \\(Y\\). We’ll start with the variance of \\(Y\\), assuming that \\(\\varepsilon\\) is independent of everything.\r\n\\[ Var(Y) = \\beta_W^2\\sigma^2_W + \\beta^2_Z\\sigma^2_Z + \\beta^2_X\\sigma^2_X + 2\\beta_W\\beta_Z\\sigma_{WZ}+2\\beta_W\\beta_X\\sigma_{WX}+2\\beta_Z\\beta_X\\sigma_{ZX}+\\sigma^2\\varepsilon \\]\r\nWe’ll take this separately conditional on \\(Z = 1\\) and \\(Z = 0\\) and differencing (under the assumption that treatment effects do not vary between \\(Z = 1\\) and \\(Z = 0\\)):\r\n\\[\\begin{eqnarray} \r\nVar(Y|Z=1) - Var(Y|Z=0) = \\beta^2_W(\\sigma^2_{W|Z=1}-\\sigma^2_{W|Z=0}) + \\beta^2_X(\\sigma^2_{X|Z=1} - \\sigma^2_{X|Z=0}) +   \\\\\r\n2\\beta_W\\beta_X(\\sigma_{WX|Z=1}-\\sigma_{WX|Z=0})\r\n\\end{eqnarray}\\]\r\nLet’s replace all those differences in variance and covariance between \\(Z = 1\\) and \\(Z = 0\\) with \\(d\\) for notational simplicity, i.e. \\(dA = Var(A|Z=1)-Var(A|Z=0)\\) and \\(dAB=Cov(A,B|Z=1)-Cov(A,B|Z=0)\\). Then rearrange.\r\n\\[ 0 = \\beta_X^2dX + 2\\beta_W\\beta_XdWX + (\\beta^2_WdW - dY) \\]\r\nThis is a quadratic in \\(\\beta_X\\) and so we can solve it with the quadratic formula.\r\n\\[ \\beta_X = \\frac{(-b\\pm  \\sqrt{b^2-4ac})}{2a} \\]\r\n\\[ \\beta_X = \\frac{(-2\\beta_WdWX\\pm  \\sqrt{(2\\beta_WdWX)^2-4dX(\\beta^2_WWdW - dY)})}{2dX} \\]\r\nAll the terms with \\(W\\) are unobservable, but the rest are observable. Let’s assume that \\(dWX=dW=0\\). That is, \\(W\\) and \\(X\\) may be correlated, but that degree of correlation is not related to \\(Z\\). Further, the variance of \\(W\\) is not related to \\(Z\\) (although potentially \\(Z\\) and \\(W\\) may be related, that’s fine). This gives us\r\n\\[ \\beta_X = \\frac{\\sqrt{dY}}{\\sqrt{dX}} \\]\r\n\\[ \\beta_X^2 = \\frac{dY}{dX} \\]\r\nThat’s riiiight, it’s a Wald estimator! Derived from the quadratic formula! In this setup, you’re basically using \\(Z\\) as an instrument for \\(\\sigma_X^2\\). That’s where the necessary assumptions come from, they’re just all about variances rather than means. It’s ok if \\(Z\\) is related to \\(W\\) just so long as \\(Z\\) is unrelated to \\(\\sigma_W\\) (the \\(dW=0\\) assumption).\r\nNow we have the magnitude of \\(\\beta_X\\) - it’s \\(\\frac{\\sqrt{dY}}{\\sqrt{dX}}\\). We don’t get the sign, but we generally have a pretty strong theoretical idea of what a sign should be, and the magnitude (or whether that magnitude is nonzero) is more the question.\r\nNotably, also, this could all be done with a discretized continuous Z, i.e. “pick two groups for which \\(\\sigma_X\\) is different, using \\(Z\\) to identify those groups”.\r\nSimulation\r\nI haven’t worked out a more nonparametric or nonlinear version of this, but that’s the basic idea. It works in simulation, too (and yes, 2SLS is an equivalent way to derive this in the linear case, and you get SEs).\r\n\r\n\r\n\r\nIn the sampling distribution you can see that the mean and median are both bang-on to the true value of 2. The standard deviation of the sampling distribution ain’t shabby either.\r\n\r\n\r\nTable 1: Summary Statistics\r\n\r\n\r\nVariable\r\n\r\n\r\nN\r\n\r\n\r\nMean\r\n\r\n\r\nStd. Dev.\r\n\r\n\r\nMin\r\n\r\n\r\nPctl. 25\r\n\r\n\r\nPctl. 50\r\n\r\n\r\nPctl. 75\r\n\r\n\r\nMax\r\n\r\n\r\nrat\r\n\r\n\r\n500\r\n\r\n\r\n1.998\r\n\r\n\r\n0.047\r\n\r\n\r\n1.824\r\n\r\n\r\n1.97\r\n\r\n\r\n1.998\r\n\r\n\r\n2.03\r\n\r\n\r\n2.131\r\n\r\n\r\n\r\n\r\n\r\nConclusion\r\nWhat can we get from all of this?\r\nWell, I think it’s kind of neat that this offers a potential way to identify a causal effect without so many of the assumptions that we typically rely on and seem impossible to get around without some sort of quasiexperimental design.\r\nAt least the way I’m thinking about it, this is basically a way of managing to control for unobserved confounders using observed confounders.\r\nOf course, that’s not entirely true. You could call this a quasiexperimental design, really. I mean, it reduces mathematically to 2SLS at least in some formulations. But the assumptions that stand in here for the exclusion restriction feel so different that it seems like a very different kind of thing.\r\nThere are the downsides, of course. The obvious one being “where the heck would we expect these assumptions to hold?” The variance of treatment is related to the value of another, sure. But we aren’t used to thinking about no-heteroskedasticity assumptions, i.e. the assumption here that the variance of the unobserved confounder \\(W\\) is unrelated to \\(Z\\) (and in a case where maybe even the mean of \\(W\\) is related to \\(Z\\)! Since if that’s not true we may as well just use regular IV).\r\nI’m not even sure the downside here is how unlikely the assumptions are so much as I’m not even sure how to reason about where I’d expect these assumptions to hold. I have too much experience working with uncorrelatedness assumptions. I’m not sure what a “the unobserved confounder’s variance is unrelated to this other variable’s value” assumption feels like or how to judge whether that assumption is plausible.\r\nSo maybe it’s just sorta neat rather than useful. This process feels a little like cheating, and I love a statistical method that feels like cheating. Plus I got to derive a Wald from the quadratic, which was fun. Then again, the different kind of assumption it calls for suggests that maybe there are situations where this applies, and I just don’t know where to look for them yet! Let me know if you think of one!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-16-identification-by-heteroskedasticity/identification-by-heteroskedasticity_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-11-16T02:39:30-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
