<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.89.2" />


<title>Identification by Heteroskedasticity - Some Thoughts I&#39;ve Had</title>
<meta property="og:title" content="Identification by Heteroskedasticity - Some Thoughts I&#39;ve Had">


  <link href='https://nickch-k.github.io/SomeThoughts/favicon.ico' rel='icon' type='image/x-icon'/>



  
  <meta property="description" content="A basic idea for identifying causal effects using heteroskedasticity">
  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/SomeThoughts/css/fonts.css" media="all">
<link rel="stylesheet" href="/SomeThoughts/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/SomeThoughts/" class="nav-logo">
    <img src="/SomeThoughts/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://github.com/NickCH-K">GitHub</a></li>
    
    <li><a href="https://www.nickchk.com">Main Site</a></li>
    
    <li><a href="https://twitter.com/nickchk">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Identification by Heteroskedasticity</h1>

    
    <span class="article-date">2021-11-12</span>
    

    <div class="article-content">
      
<script src="https://nickch-k.github.io/SomeThoughts/2021/11/12/identification-by-heteroskedasticity/index_files/header-attrs/header-attrs.js"></script>
<script src="https://nickch-k.github.io/SomeThoughts/2021/11/12/identification-by-heteroskedasticity/index_files/kePrint/kePrint.js"></script>
<link href="https://nickch-k.github.io/SomeThoughts/2021/11/12/identification-by-heteroskedasticity/index_files/lightable/lightable.css" rel="stylesheet" />


<p><em>Since posting this on Twitter I have learned that this idea is in fact approximately 20 years old, and has had a number of papers by Lewbel. Ah, and yet.</em></p>
<p>In this article I will produce a sketch to demonstrate an entirely new approach to identifying a causal effect that relies on an entirely different set of assumptions from what you’d normally see. Why am I introducing this via blog rather than in an academic publication? Simply, if I put the time into developing this into an academic publication, I can tell you exactly what the reviewers would say: “okay, sure, but in what context do these different assumptions actually hold?” and I’d give a big ol’ shrug because I don’t know. I still think it’s pretty neat if only as a curiosity. And hey, if you’re out there and thinking “hold on, I <em>do</em> actually know a context where this can be applied!” then please do get in contact with me. I’d love to hear about it, and maybe then I’d turn this into a paper.</p>
<p>At this point, it’s really just an idea.</p>
<p>The basic idea begins with the basic way we usually identify the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, which is through the use of assumptions about variables being <em>unrelated</em>. You almost always need one (or more) of these assumptions to be able to identify a causal effect. If you want to simply estimate the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and say “this relationship describes the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>” you need to make an assumption similar to “the other determinants of <span class="math inline">\(Y\)</span> that are not themselves determined by <span class="math inline">\(X\)</span> are all unrelated to <span class="math inline">\(X\)</span>”.</p>
<p>This assumption is actually stronger than we need it to be. What we actually need, in a regression for example, is that variables are <em>uncorrelated</em>. This is not as strong a requirement as being unrelated, since “uncorrelated” only cares about linear dependence. If the graph with <span class="math inline">\(Y\)</span> on the y-axis and <span class="math inline">\(X\)</span> on the x-axis has the shape of a U, for example, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are clearly related, but are uncorrelated.</p>
<p>We’re used to assumptions about things being uncorrelated. When thinking through a causal analysis that uses regression, we constantly ask whether the mean of the treatment variable <span class="math inline">\(X\)</span> is likely to be higher or lower based on the value of some variable that’s been excluded from the regression.</p>
<p>We think about other elements of two variables being related besides correlation in some ways too. We think about <em>heteroskedasticity</em>, which is the relationship between <span class="math inline">\(X\)</span> and the <em>variance</em> of the error term (which consists of terms excluded from the regression). Heteroskedasticity isn’t important for identifying the effect, but we do need to keep it in mind when calculating standard errors.</p>
<p>But what if we could flip that? What if assumptions about <em>heteroskedasticity</em> were what we used to identify our effect, and assumptions about <em>correlation</em> weren’t important?</p>
<p>This is possible under the right assumptions!</p>
<div id="the-basic-idea" class="section level2">
<h2>The Basic Idea</h2>
<p>The insight here is that heteroskedasticity allows you to pick up a causal effect (or at least its magnitude) in a sort of dose-response kind of way. Intuitively, if you <em>make the treatment more noisy</em>, then if there’s a causal effect, the <em>outcome should get more noisy at the same time</em>. We can detect the extent to which this happens by comparing variances. That’s it, that’s the idea.</p>
</div>
<div id="a-mathematical-demonstration" class="section level2">
<h2>A Mathematical Demonstration</h2>
<p>Let’s do a demonstration. We are trying to get the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, both continuous and mean-zero. However, there are two confounders, one binary observed confounder <span class="math inline">\(Z\)</span> and one unobserved confounder <span class="math inline">\(W\)</span>. The data generating process is as follows:</p>
<p><span class="math display">\[ X = \gamma_z Z + \gamma_wW + \nu \]</span></p>
<p><span class="math display">\[ Y = \beta_x X + \beta_z Z + \gamma_wW + \varepsilon \]</span></p>
<p><span class="math display">\[ \nu \sim N(0, \sigma^2_X \equiv 1 + Z\theta_{xz} + W\theta_{xw}), \varepsilon \sim N(0, \sigma^2_\varepsilon) \]</span></p>
<p>If we use a method that relies on <span class="math inline">\(E(XW)=E(X)E(W)\)</span>, like regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> and looking at the coefficient on <span class="math inline">\(X\)</span>, we’ll be biased, since <span class="math inline">\(X\)</span> and <span class="math inline">\(W\)</span> are correlated.</p>
<p>But what if, instead, we look separately at the <span class="math inline">\(Z = 0\)</span> and <span class="math inline">\(Z = 1\)</span> groups. Then, in those groups, we examine the variance of <span class="math inline">\(Y\)</span>. We’ll start with the variance of <span class="math inline">\(Y\)</span>, assuming that <span class="math inline">\(\varepsilon\)</span> is independent of everything.</p>
<p><span class="math display">\[ Var(Y) = \beta_W^2\sigma^2_W + \beta^2_Z\sigma^2_Z + \beta^2_X\sigma^2_X + 2\beta_W\beta_Z\sigma_{WZ}+2\beta_W\beta_X\sigma_{WX}+2\beta_Z\beta_X\sigma_{ZX}+\sigma^2\varepsilon \]</span></p>
<p>We’ll take this separately conditional on <span class="math inline">\(Z = 1\)</span> and <span class="math inline">\(Z = 0\)</span> and differencing (under the assumption that treatment effects do not vary between <span class="math inline">\(Z = 1\)</span> and <span class="math inline">\(Z = 0\)</span>):</p>
<p><span class="math display">\[\begin{eqnarray} 
Var(Y|Z=1) - Var(Y|Z=0) = \beta^2_W(\sigma^2_{W|Z=1}-\sigma^2_{W|Z=0}) + \beta^2_X(\sigma^2_{X|Z=1} - \sigma^2_{X|Z=0}) +   \\
2\beta_W\beta_X(\sigma_{WX|Z=1}-\sigma_{WX|Z=0})
\end{eqnarray}\]</span></p>
<p>Let’s replace all those differences in variance and covariance between <span class="math inline">\(Z = 1\)</span> and <span class="math inline">\(Z = 0\)</span> with <span class="math inline">\(d\)</span> for notational simplicity, i.e. <span class="math inline">\(dA = Var(A|Z=1)-Var(A|Z=0)\)</span> and <span class="math inline">\(dAB=Cov(A,B|Z=1)-Cov(A,B|Z=0)\)</span>. Then rearrange.</p>
<p><span class="math display">\[ 0 = \beta_X^2dX + 2\beta_W\beta_XdWX + (\beta^2_WdW - dY) \]</span></p>
<p>This is a quadratic in <span class="math inline">\(\beta_X\)</span> and so we can solve it with the quadratic formula.</p>
<p><span class="math display">\[ \beta_X = \frac{(-b\pm  \sqrt{b^2-4ac})}{2a} \]</span></p>
<p><span class="math display">\[ \beta_X = \frac{(-2\beta_WdWX\pm  \sqrt{(2\beta_WdWX)^2-4dX(\beta^2_WWdW - dY)})}{2dX} \]</span></p>
<p>All the terms with <span class="math inline">\(W\)</span> are unobservable, but the rest are observable. Let’s assume that <span class="math inline">\(dWX=dW=0\)</span>. That is, <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span> may be correlated, but that degree of correlation is not related to <span class="math inline">\(Z\)</span>. Further, the variance of <span class="math inline">\(W\)</span> is not related to <span class="math inline">\(Z\)</span> (although potentially <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> may be related, that’s fine). This gives us</p>
<p><span class="math display">\[ \beta_X = \frac{\sqrt{dY}}{\sqrt{dX}} \]</span></p>
<p><span class="math display">\[ \beta_X^2 = \frac{dY}{dX} \]</span></p>
<p>That’s riiiight, it’s a Wald estimator! Derived from the quadratic formula! In this setup, you’re basically using <span class="math inline">\(Z\)</span> as an instrument for <span class="math inline">\(\sigma_X^2\)</span>. That’s where the necessary assumptions come from, they’re just all about <em>variances</em> rather than means. It’s ok if <span class="math inline">\(Z\)</span> is related to <span class="math inline">\(W\)</span> just so long as <span class="math inline">\(Z\)</span> is unrelated to <span class="math inline">\(\sigma_W\)</span> (the <span class="math inline">\(dW=0\)</span> assumption).</p>
<p>Now we have the magnitude of <span class="math inline">\(\beta_X\)</span> - it’s <span class="math inline">\(\frac{\sqrt{dY}}{\sqrt{dX}}\)</span>. We don’t get the sign, but we generally have a pretty strong theoretical idea of what a sign should be, and the magnitude (or whether that magnitude is nonzero) is more the question.</p>
<p>Notably, also, this could all be done with a discretized continuous Z, i.e. “pick two groups for which <span class="math inline">\(\sigma_X\)</span> is different, using <span class="math inline">\(Z\)</span> to identify those groups”.</p>
</div>
<div id="simulation" class="section level2">
<h2>Simulation</h2>
<p>I haven’t worked out a more nonparametric or nonlinear version of this, but that’s the basic idea. It works in simulation, too (and yes, 2SLS is an equivalent way to derive this in the linear case, and you get SEs).</p>
<pre class="r"><code>{
  library(purrr)
  library(data.table)
  library(vtable)
  library(ggplot2)
}


# Function to create simulated data
create_data &lt;- function(N=1000, effect, WX, WZ) {
  # Our unobserved confounder, let&#39;s make it uniform why not
  d &lt;- data.table(W = runif(N, 0, 2))
  # Our 
  d[, Z := runif(N) &gt; .5]
  # Treatment mean and variance are both related to W and Z
  d[, treat := rnorm(N, mean = WX*W + WZ*Z, sd=W + Z)]
  # True model with true effect &quot;effect&quot;
  d[, Y := W + effect*treat + Z + rnorm(N)]
  return(d)
}

# Function to estimate the model
est_model &lt;- function(i,N=1000, effect = 2, WX = 1, WZ = 1) {
  d &lt;- create_data(N, effect, WX, WZ)
  # Calculate variance of Y and treatment at Z = 1 and Z = 0
  return(list(Ytop = var(d[Z==1]$Y),
              Ybot = var(d[Z==0]$Y),
              ttop = var(d[Z==1]$treat),
              tbot = var(d[Z==0]$treat)))
}

# Run the model 500 times
set.seed(1000)
res &lt;- 1:500 %&gt;%
  map_df(est_model) %&gt;%
  as.data.table()

# Calculate the Wald estimator for each iteration
res[, rat := sqrt((Ytop-Ybot)/(ttop-tbot))]</code></pre>
<p>In the sampling distribution you can see that the mean and median are both bang-on to the true value of 2. The standard deviation of the sampling distribution ain’t shabby either.</p>
<pre class="r"><code>sumtable(res, vars = &#39;rat&#39;, add.median = TRUE)</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-2">Table 1: </span>Summary Statistics
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
N
</th>
<th style="text-align:left;">
Mean
</th>
<th style="text-align:left;">
Std. Dev.
</th>
<th style="text-align:left;">
Min
</th>
<th style="text-align:left;">
Pctl. 25
</th>
<th style="text-align:left;">
Pctl. 50
</th>
<th style="text-align:left;">
Pctl. 75
</th>
<th style="text-align:left;">
Max
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rat
</td>
<td style="text-align:left;">
500
</td>
<td style="text-align:left;">
1.998
</td>
<td style="text-align:left;">
0.047
</td>
<td style="text-align:left;">
1.824
</td>
<td style="text-align:left;">
1.97
</td>
<td style="text-align:left;">
1.998
</td>
<td style="text-align:left;">
2.03
</td>
<td style="text-align:left;">
2.131
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggplot(res, aes(x = rat)) + geom_density() + 
  geom_vline(aes(xintercept = 2), linetype = &#39;dashed&#39;) +
  annotate(x = 2, y = 2, label = &#39;True Value&#39;, geom = &#39;label&#39;) +
  theme_classic() + 
  labs(x = &#39;Estimate&#39;, y = &#39;Density&#39;)</code></pre>
<p><img src="https://nickch-k.github.io/SomeThoughts/2021/11/12/identification-by-heteroskedasticity/index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>What can we get from all of this?</p>
<p>Well, I think it’s kind of neat that this offers a potential way to identify a causal effect without so many of the assumptions that we typically rely on and seem impossible to get around without some sort of quasiexperimental design.</p>
<p>At least the way I’m thinking about it, this is basically a way of managing to control for unobserved confounders using observed confounders.</p>
<p>Of course, that’s not entirely true. You could call this a quasiexperimental design, really. I mean, it reduces mathematically to 2SLS at least in some formulations. But the assumptions that stand in here for the exclusion restriction feel so different that it seems like a very different kind of thing.</p>
<p>There are the downsides, of course. The obvious one being “where the heck would we expect these assumptions to hold?” The variance of treatment is related to the value of another, sure. But we aren’t used to thinking about <em>no</em>-heteroskedasticity assumptions, i.e. the assumption here that the variance of the unobserved confounder <span class="math inline">\(W\)</span> is unrelated to <span class="math inline">\(Z\)</span> (and in a case where maybe even the mean of <span class="math inline">\(W\)</span> <em>is</em> related to <span class="math inline">\(Z\)</span>! Since if that’s not true we may as well just use <em>regular</em> IV).</p>
<p>I’m not even sure the downside here is how unlikely the assumptions are so much as <em>I’m not even sure how to reason about where I’d expect these assumptions to hold</em>. I have too much experience working with uncorrelatedness assumptions. I’m not sure what a “the unobserved confounder’s variance is unrelated to this other variable’s value” assumption <em>feels</em> like or how to judge whether that assumption is plausible.</p>
<p>So maybe it’s just sorta neat rather than useful. This process feels a little like cheating, and I love a statistical method that feels like cheating. Plus I got to derive a Wald from the quadratic, which was fun. Then again, the different kind of assumption it calls for suggests that maybe there <em>are</em> situations where this applies, and I just don’t know where to look for them yet! Let me know if you think of one!</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/SomeThoughts/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/SomeThoughts/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/SomeThoughts/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

